{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99cc021d-2568-4012-9377-987ee3de600a",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "1. In your terminal, cd in to `tutorials_deeplearninghero/llms`\n",
    "2. Clone the Mini-GPT4 repo with `git clone https://github.com/Vision-CAIR/MiniGPT-4.git`\n",
    "3. `cd` into `MiniGPT-4` and create the conda environment with `conda env create -f environment.yml`\n",
    "4. Activate the environment `conda activate minigpt4`\n",
    "5. Install `ipykernel` with `conda install ipykernel`\n",
    "6. Install the kernel with `ipython kernel install --name \"minigpt4\" --user`\n",
    "7. Make sure thie `minigpt4` kernel is selected for your notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd914c24-3b09-4860-8678-8ae0e10b1888",
   "metadata": {},
   "source": [
    "## Install few more libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d06e4b3-3520-417d-9e47-1a96baba0e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "!/home/user/conda/envs/minigpt4/bin/pip install --quiet fschat==0.1.10 gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bb9f7e2-1b4a-4248-8df0-6788ebe817b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/conda/envs/minigpt4/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import pathlib\n",
    "import os\n",
    "import gdown\n",
    "import transformers\n",
    "import gc\n",
    "import huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abba3cb9-9611-46a9-93dd-8815dc82099e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setting up Mini-GPT4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75e31191-4670-484f-8f79-5cf55aed6d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# It looks like using ~/.cache as opposed to absolute /home/jupyter it points to somehwere else\n",
    "# Figure out where it points to\n",
    "default_cache_dir = pathlib.Path(\"/home/jupyter/.cache/huggingface/hub\")\n",
    "llama_space = \"decapoda-research\"\n",
    "llama_id = \"llama-7b-hf\"\n",
    "vicuna_space = \"lmsys\"\n",
    "vicuna_id = \"vicuna-7b-delta-v0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840af4fd-f5fe-482d-8a62-24fc41c329b2",
   "metadata": {},
   "source": [
    "## Download base models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0593272-592c-43de-b666-5a0547868403",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 42 files: 100%|██████████| 42/42 [00:00<00:00, 2163.71it/s]\n",
      "Fetching 10 files: 100%|██████████| 10/10 [00:00<00:00, 1736.20it/s]\n"
     ]
    }
   ],
   "source": [
    "def download_models():\n",
    "    llama_repo_id = f\"{llama_space}/{llama_id}\"\n",
    "    vicuna_repo_id = f\"{vicuna_space}/{vicuna_id}\"\n",
    "    huggingface_hub.snapshot_download(repo_id=llama_repo_id)\n",
    "    huggingface_hub.snapshot_download(repo_id=vicuna_repo_id)\n",
    "      \n",
    "download_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e5a1a8f-0fd0-4cb2-aab5-fafe3012996c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def patch_tokenizer_config(default_cache_dir):\n",
    "    # исправление  https://github.com/huggingface/transformers/issues/22222#issuecomment-1477171703\n",
    "    for space, repo in [(vicuna_space, vicuna_id), (llama_space, llama_id)]:\n",
    "        for path in pathlib.Path(default_cache_dir / f\"models--{space}--{repo}/snapshots/\").rglob(\"*/tokenizer_config.json\"):\n",
    "            print(f\"Loading {path}\")\n",
    "            config = json.loads(open(path, \"r\").read())\n",
    "            if config[\"tokenizer_class\"] == \"LlamaTokenizer\":\n",
    "                print(\"No fix needed\")\n",
    "            else:\n",
    "                config[\"tokenizer_class\"] = \"LlamaTokenizer\"\n",
    "            with open(path, \"w\") as f:\n",
    "                json.dump(config, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d6f2e4e1-327f-4f97-8603-2d176ba166ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_tokenizer_config(default_cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eee4ee5d-79a0-4d4f-869c-254d9fe32fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available objects for config:\n",
      "    AliasManager\n",
      "    DisplayFormatter\n",
      "    HistoryManager\n",
      "    IPCompleter\n",
      "    IPKernelApp\n",
      "    LoggingMagics\n",
      "    MagicsManager\n",
      "    OSMagics\n",
      "    PrefilterManager\n",
      "    ScriptMagics\n",
      "    StoreMagics\n",
      "    ZMQInteractiveShell\n"
     ]
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e053bd7-03e0-443f-ae8e-d8b7cfeb8842",
   "metadata": {},
   "source": [
    "## Applying Vicuna deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "352ee1b3-707a-46fb-b29f-91d7dfa8c798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vicuna weights are deltas which needs to be applied on top of llama\n",
    "!/home/user/conda/envs/minigpt4/bin/python -m fastchat.model.apply_delta \\\n",
    "    --base-model-path $default_cache_dir/models--$llama_space--$llama_id/snapshots/*/ \\\n",
    "    --target-model-path ./vicuna-7b-v0 \\\n",
    "    --delta-path $default_cache_dir/models--$vicuna_space--$vicuna_id/snapshots/*/ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ad9d01-49c8-4d2b-b2f2-e9165caeb969",
   "metadata": {},
   "source": [
    "## Загрузим BLIP-2 checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ad74823-10ed-42fd-8cf0-a6f4f4d5195e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (uriginal): https://drive.google.com/uc?id=1RY9jV0dyqLX-o38LrumkKRh6Jtaop58R\n",
      "From (redirected): https://drive.google.com/uc?id=1RY9jV0dyqLX-o38LrumkKRh6Jtaop58R&confirm=t&uuid=fff28f69-c2ba-48ed-9d76-b102bfcb6fb5\n",
      "To: /home/jovyan/arseny/tutorials_deeplearninghero/llms/pretrained_minigpt4.pth\n",
      "100%|██████████| 37.9M/37.9M [00:19<00:00, 1.97MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'pretrained_minigpt4.pth'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_path = 'pretrained_minigpt4.pth'\n",
    "gdown.download(\n",
    "    \"https://drive.google.com/file/d/1RY9jV0dyqLX-o38LrumkKRh6Jtaop58R/view?usp=sharing\", output_path, fuzzy=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ebfab2d-d000-4d59-9c96-9ebd2ae8e626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100 34.4M    0 34.4M    0     0  17.6M      0 --:--:--  0:00:01 --:--:-- 24.7M\n"
     ]
    }
   ],
   "source": [
    "#!curl -LO https://github.com/Vision-CAIR/MiniGPT-4/archive/refs/heads/main.zip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9059319d-8729-4a87-a45a-9eb7a54b9660",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import zipfile\n",
    "#with zipfile.ZipFile(\"main.zip\", 'r') as zip_ref:\n",
    "#    zip_ref.extractall(\"./\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d6bb6f-f67a-4c65-8542-bc5acd222bcd",
   "metadata": {},
   "source": [
    "## Setting paths to configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6ab06729-4bff-49eb-a2e9-eee781ddeb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "eval_config_path = pathlib.Path(\"MiniGPT-4/eval_configs/minigpt4_eval.yaml\")\n",
    "with open(eval_config_path, \"r\") as f:\n",
    "    eval_config_dict = yaml.safe_load(f)\n",
    "    eval_config_dict[\"model\"][\"ckpt\"] = \"./pretrained_minigpt4.pth\"\n",
    "    eval_config_dict[\"model\"][\"prompt_path\"] = \"./MiniGPT-4/prompts/alignment.txt\"\n",
    "    \n",
    "with open(eval_config_path, \"w\") as f:\n",
    "    yaml.dump(eval_config_dict, f)\n",
    "\n",
    "minigpt4_config_path = pathlib.Path(\"MiniGPT-4/minigpt4/configs/models/minigpt4.yaml\")\n",
    "with open(minigpt4_config_path, \"r\") as f:\n",
    "    minigpt4_config_dict = yaml.safe_load(f)\n",
    "    minigpt4_config_dict[\"model\"][\"llama_model\"] = \"./vicuna-7b-v0\"\n",
    "    \n",
    "with open(minigpt4_config_path, \"w\") as f:\n",
    "    yaml.dump(minigpt4_config_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53880870-3b96-4baf-b35b-d75e9d4bea22",
   "metadata": {},
   "source": [
    "## Running Mini-GPT4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e42ec8c7-9dc2-408a-83f8-89ac8900d609",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "minigpt4_path = './MiniGPT-4'\n",
    "if sys.path[-1] != minigpt4_path:\n",
    "    sys.path.append(minigpt4_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99e4cd6-abff-4226-8b96-251358268dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse \n",
    "from minigpt4.common.config import Config\n",
    "from minigpt4.common.registry import registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "84b62dbd-9541-47a6-a6a5-34d8cbd30ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minigpt4.datasets.builders import *\n",
    "from minigpt4.models import *\n",
    "from minigpt4.processors import *\n",
    "from minigpt4.runners import *\n",
    "from minigpt4.tasks import *\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"\")\n",
    "parser.add_argument('--cfg-path', help='')\n",
    "parser.add_argument('--options', nargs=\"+\",help='')\n",
    "parser.add_argument('--gpu-id', default=0, help='')\n",
    "args = parser.parse_args(\" --cfg-path ./MiniGPT-4/eval_configs/minigpt4_eval.yaml\".split())\n",
    "\n",
    "cfg = Config(args)\n",
    "\n",
    "model_config = cfg.model_cfg\n",
    "model_config.device_8bit = args.gpu_id\n",
    "model_cls = registry.get_model_class(model_config.arch)\n",
    "model = model_cls.from_config(model_config).to('cuda:{}'.format(args.gpu_id))\n",
    "\n",
    "vis_processor_cfg = cfg.datasets_cfg.cc_sbu_align.vis_processor.train\n",
    "vis_processor = registry.get_processor_class(vis_processor_cfg.name).from_config(vis_processor_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ac2cb32-b593-4a54-a2d1-44c7ef28c5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, LlamaTokenizer\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "from minigpt4.conversation.conversation import *\n",
    "\n",
    "\n",
    "class MiniGPT4Chat:\n",
    "    \n",
    "    def __init__(self, model, vis_processor, device='cuda:0'):\n",
    "        self.device = device\n",
    "        self.model = model\n",
    "        self.vis_processor = vis_processor\n",
    "        stop_words_ids = [torch.tensor([835]).to(self.device),\n",
    "                          torch.tensor([2277, 29937]).to(self.device)]  # '###' can be encoded in two different ways.\n",
    "        self.stopping_criteria = StoppingCriteriaList([StoppingCriteriaSub(stops=stop_words_ids)])\n",
    "        self.conv, self.img_list = None, None\n",
    "        self.reset_history()\n",
    "        \n",
    "    def ask(self, text):\n",
    "        if len(self.conv.messages) > 0 and self.conv.messages[-1][0] == self.conv.roles[0] \\\n",
    "                and self.conv.messages[-1][1][-6:] == '</Img>':  # last message is image.\n",
    "            self.conv.messages[-1][1] = ' '.join([self.conv.messages[-1][1], text])\n",
    "        else:\n",
    "            self.conv.append_message(self.conv.roles[0], text)\n",
    "\n",
    "    def answer(self, max_new_tokens=300, num_beams=1, min_length=1, top_p=0.9,\n",
    "               repetition_penalty=1.0, length_penalty=1, temperature=1.0, max_length=2000):\n",
    "        self.conv.append_message(self.conv.roles[1], None)\n",
    "        embs = self.get_context_emb(self.img_list)\n",
    "\n",
    "        current_max_len = embs.shape[1] + max_new_tokens\n",
    "        if current_max_len - max_length > 0:\n",
    "            print('Warning: The number of tokens in current conversation exceeds the max length. '\n",
    "                  'The model will not see the contexts outside the range.')\n",
    "        begin_idx = max(0, current_max_len - max_length)\n",
    "\n",
    "        embs = embs[:, begin_idx:]\n",
    "\n",
    "        outputs = self.model.llama_model.generate(\n",
    "            inputs_embeds=embs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            stopping_criteria=self.stopping_criteria,\n",
    "            num_beams=num_beams,\n",
    "            do_sample=True if num_beams==1 else False,\n",
    "            min_length=min_length,\n",
    "            top_p=top_p,\n",
    "            repetition_penalty=repetition_penalty,\n",
    "            length_penalty=length_penalty,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "        output_token = outputs[0]\n",
    "        if output_token[0] == 0:  # the model might output a unknow token <unk> at the beginning. remove it\n",
    "            output_token = output_token[1:]\n",
    "        if output_token[0] == 1:  # some users find that there is a start token <s> at the beginning. remove it\n",
    "            output_token = output_token[1:]\n",
    "        output_text = self.model.llama_tokenizer.decode(output_token, add_special_tokens=False)\n",
    "        output_text = output_text.split('###')[0]  # remove the stop sign '###'\n",
    "        output_text = output_text.split('Assistant:')[-1].strip()\n",
    "        self.conv.messages[-1][1] = output_text\n",
    "        return output_text, output_token.cpu().numpy()\n",
    "\n",
    "    def upload_img(self, image):\n",
    "        if isinstance(image, str):  # is a image path\n",
    "            raw_image = Image.open(image).convert('RGB')\n",
    "            image = self.vis_processor(raw_image).unsqueeze(0).to(self.device)\n",
    "        elif isinstance(image, Image.Image):\n",
    "            raw_image = image\n",
    "            image = self.vis_processor(raw_image).unsqueeze(0).to(self.device)\n",
    "        elif isinstance(image, torch.Tensor):\n",
    "            if len(image.shape) == 3:\n",
    "                image = image.unsqueeze(0)\n",
    "            image = image.to(self.device)\n",
    "\n",
    "        image_emb, _ = self.model.encode_img(image)\n",
    "        self.img_list.append(image_emb)\n",
    "        self.conv.append_message(self.conv.roles[0], \"<Img><ImageHere></Img>\")\n",
    "        msg = \"Received.\"\n",
    "        return msg\n",
    "\n",
    "    def get_context_emb(self, img_list):\n",
    "        prompt = self.conv.get_prompt()\n",
    "        prompt_segs = prompt.split('<ImageHere>')\n",
    "        assert len(prompt_segs) == len(img_list) + 1, \"Unmatched numbers of image placeholders and images.\"\n",
    "        seg_tokens = [\n",
    "            self.model.llama_tokenizer(\n",
    "                seg, return_tensors=\"pt\", add_special_tokens=i == 0).to(self.device).input_ids\n",
    "            # only add bos to the first seg\n",
    "            for i, seg in enumerate(prompt_segs)\n",
    "        ]\n",
    "        seg_embs = [self.model.llama_model.model.embed_tokens(seg_t) for seg_t in seg_tokens]\n",
    "        mixed_embs = [emb for pair in zip(seg_embs[:-1], img_list) for emb in pair] + [seg_embs[-1]]\n",
    "        mixed_embs = torch.cat(mixed_embs, dim=1)\n",
    "        return mixed_embs\n",
    "    \n",
    "    def reset_history(self):\n",
    "        self.conv = Conversation(\n",
    "            system=\"Give the following image: <Img>ImageContent</Img>. \"\n",
    "                   \"You will be able to see the image once I provide it to you. Please answer my questions.\",\n",
    "            roles=(\"Human\", \"Assistant\"),\n",
    "            messages=[],\n",
    "            offset=2,\n",
    "            sep_style=SeparatorStyle.SINGLE,\n",
    "            sep=\"###\",\n",
    "        )\n",
    "        self.img_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704124c0-f355-4fa5-b47e-938a296388ec",
   "metadata": {},
   "source": [
    "## Инференс MiniGPT4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "30bc0514-f3ee-4870-879c-640e19b6b080",
   "metadata": {},
   "outputs": [],
   "source": [
    "thumbnail_paths = [\n",
    "    \"./images/cake.jpg\", \n",
    "    \"./images/ad.png\", \n",
    "    \"./images/logo.jpg\", \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5e085a57-abcc-491b-8756-6203afd34613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./images/cake.jpg :\n",
      "This image shows a chocolate cake with chocolate frosting and chocolate drizzle on top. It is on a cake stand on a white plate. The cake appears to be made with a chocolate cake mix and chocolate frosting, and is decorated with chocolate drizzle.\n",
      "--------------------\n",
      "./images/ad.png :\n",
      "This is a billboard advertisement for a dental care company called Brushes at the World. The advertisement features a woman with a mask on her face, smiling and holding a toothbrush. The tagline reads, \" Greatest ad of 2020! Get brushed at the world.\" The billboard's message is that the company, Brushes at the World, is the best place to get dental care, and that the advertisement is funny and clever.\n",
      "--------------------\n",
      "./images/logo.jpg :\n",
      "The main colors of this design are purple, pink, and green. The design is visually appealing because of the use of vibrant colors, the curves and movement in the butterfly's wings, and the overall composition of the logo.\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "prompts = {\n",
    "    \"./images/cake.jpg\": \"What are the ingredients? How do I make this?\",\n",
    "    \"./images/ad.png\": \"Explain to me why this is a clever and funny advertisement\",\n",
    "    \"./images/logo.jpg\": \"What are the main colors of this design? Is this a visually appealing design? Why?\"\n",
    "}\n",
    "\n",
    "minigpt4 = MiniGPT4Chat(model, vis_processor)\n",
    "num_beams = 1\n",
    "temperature = 0.9\n",
    "max_new_tokens = 200\n",
    "\n",
    "for path, prompt in prompts.items():\n",
    "    minigpt4.reset_history()\n",
    "    \n",
    "    minigpt4.upload_img(path)\n",
    "    minigpt4.ask(prompt)\n",
    "    out, _ = minigpt4.answer(\n",
    "        num_beams=num_beams,\n",
    "        temperature=temperature,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "    )    \n",
    "    \n",
    "    print(path,\":\")\n",
    "    print(out)\n",
    "    print('-'*20)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "beda7b58-8fdc-4270-b74e-46a20d2ad513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing Chat\n",
      "Loading VIT\n",
      "Loading VIT Done\n",
      "Loading Q-Former\n",
      "Loading Q-Former Done\n",
      "Loading LLAMA\n",
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:18<00:00,  9.21s/it]\n",
      "Loading LLAMA Done\n",
      "Load 4 training prompts\n",
      "Prompt Example \n",
      "###Human: <Img><ImageHere></Img> Could you describe the contents of this image for me? ###Assistant: \n",
      "Load BLIP2-LLM Checkpoint: ./pretrained_minigpt4.pth\n",
      "Initialization Finished\n",
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://04f234d5480077b379.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n",
      "/app/search/recommendation_server/models/template_embed_contrastive_trainer/torch_wrapper_layer.runfiles/pypi_transformers/site-packages/transformers/generation/utils.py:1219: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n",
      "^C\n",
      "Keyboard interruption in main thread... closing server.\n",
      "Killing tunnel 127.0.0.1:7860 <> https://04f234d5480077b379.gradio.live\n"
     ]
    }
   ],
   "source": [
    "#!python MiniGPT-4/demo.py --cfg-path MiniGPT-4/eval_configs/minigpt4_eval.yaml  --gpu-id 0"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "minigpt4",
   "name": "common-cu110.m108",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m108"
  },
  "kernelspec": {
   "display_name": "minigpt4",
   "language": "python",
   "name": "minigpt4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
